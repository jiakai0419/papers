# Papers


### Bidding Strategy
* [Lift-Based Bidding in Ad Selection](https://arxiv.org/pdf/1507.04811.pdf) 2016
* [Attribution Modeling Increases Efficiency of Bidding in Display Advertising](https://arxiv.org/pdf/1707.06409.pdf) 2017
* [Combining Powers of Two Predictors in Optimizing Real-Time Bidding Strategy under Constrained Budget](http://wnzhang.net/share/rtb-papers/two-pred-bid.pdf) 2016

### Pacing
* [Real Time Bid Optimization with Smooth Budget Delivery in Online Advertising](https://arxiv.org/pdf/1305.3011.pdf) 2013

### Reinforcement Learning
* [LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions](https://arxiv.org/pdf/1708.05565.pdf) 2017
* [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf) 2016
* [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf) 2013
* [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf) 2015
* [Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) 2015
* [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf) 2014
* [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf) 2017
* [Conservative Q-Learning for Offline Reinforcement Learning](https://papers.nips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf) 2020

### Model
* [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) 2010
* [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf) 2016
* [Large Scale Distributed Deep Networks](https://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf) 2012
* [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247.pdf) 2017
* [DeepFM: An End-to-End Wide & Deep Learning Framework for CTR Prediction](https://arxiv.org/pdf/1804.04950.pdf) 2018
* [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/pdf/2008.13535.pdf) 2020
* [Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/pdf/1706.06978.pdf) 2018

### Activation
* [Activate or Not: Learning Customized Activation](https://arxiv.org/pdf/2009.04759.pdf) 2021

### TensorFlow
* [TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems](https://arxiv.org/pdf/1603.04467.pdf) 2016
* [TensorFlow: A System for Large-Scale Machine Learning](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf) 2016
* [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](http://stevenwhang.com/tfx_paper.pdf) 2017

### Retrieval
* [Learning Tree-based Deep Model for Recommender Systems](https://arxiv.org/pdf/1801.02294.pdf) 2018
* [Joint Optimization of Tree-based Index and Deep Model for Recommender Systems](https://arxiv.org/pdf/1902.07565.pdf) 2019
* [Learning Optimal Tree Models under Beam Search](https://proceedings.icml.cc/static/paper_files/icml/2020/2514-Paper.pdf) 2020
* [Product quantization for nearest neighbor search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) 2010
* [Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf) 2019
* [Path-based Deep Network for Candidate Item Matching in Recommenders](https://arxiv.org/pdf/2105.08246.pdf) 2021
* [Beyond User Embedding Matrix: Learning to Hash for Modeling Large-Scale Users in Recommendation](http://www.thuir.cn/group/~mzhang/publications/SIGIR2020-ShiShaoyun.pdf) 2020
* [Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach](https://arxiv.org/pdf/2105.07706.pdf) 2021

### LTR
* [A Short Introduction to Learning to Rank](http://times.cs.uiuc.edu/course/598f14/l2r.pdf) 2011

### Model Compression and Acceleration
* [8-bit Inference with TensorRT](http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf) 2017
* [PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS](https://arxiv.org/pdf/1805.06085.pdf) 2018
* [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/pdf/1712.05877.pdf) 2017
* [DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/pdf/1510.00149.pdf) 2016
* [Improving the speed of neural networks on CPUs](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37631.pdf) 2011
* [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf) 2015
* [Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1](https://arxiv.org/pdf/1602.02830.pdf) 2016
* [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/pdf/1603.05279.pdf) 2016
* [A Review of Binarized Neural Networks](https://www.mdpi.com/2079-9292/8/6/661/htm) 2019
* [BNN+: IMPROVED BINARY NETWORK TRAINING](https://openreview.net/pdf?id=SJfHg2A5tQ) 2019
* [What Do Compressed Deep Neural Networks Forget?](https://arxiv.org/pdf/1911.05248.pdf) 2020
* [Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://arxiv.org/pdf/1912.05671.pdf) 2020
* [How to Train a Compact Binary Neural Network with High Accuracy?](https://ojs.aaai.org/index.php/AAAI/article/view/10862) 2017
* [Towards Accurate Binary Convolutional Neural Network](https://arxiv.org/pdf/1711.11294.pdf) 2017
* [Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs](https://dl.acm.org/doi/pdf/10.1145/3352460.3358269) 2019
* [Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better](https://arxiv.org/pdf/2106.08962.pdf) 2021
* [Learning to Embed Categorical Features without Embedding Tables for Recommendation](https://arxiv.org/pdf/2010.10784.pdf) 2021
* [OPTIMIZING CUDA APPLICATIONS FOR THE VOLTA/TURING ARCHITECTURE](https://on-demand.gputechconf.com/gtc-il/2018/pdf/sil8140-optimizing-cuda-applications-for-the-volta-turing-gpu-architecture.pdf) 2018
* [Making the Most of Structured Sparsity in the NVIDIA Ampere Architecture](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31552/) 2021
* [Multiplying Matrices Without Multiplying](https://arxiv.org/pdf/2106.10860.pdf) 2021

### Geometry
* [Hyperbolic Neural Networks](https://arxiv.org/pdf/1805.09112.pdf) 2018

### Meta Learning
* [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/pdf/1703.03400.pdf) 2017

### Transfer Learning
* [Domain-Adversarial Training of Neural Networks](https://arxiv.org/pdf/1505.07818.pdf) 2016

### Black-Box Optimization
* [Google Vizier: A Service for Black-Box Optimization](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf) 2017

### LLM
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 2017.06
* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) 2018.06
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) 2018.10
* [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) 2022.10
* [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201) 2023.05
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) 2022.01
* [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE]() 2023.07
* [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221) 2022.07
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) 2023.07
* [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) 2023.04
* [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) 2023.05
* [Scaling Laws and Interpretability of Learning from Repeated Data](https://arxiv.org/abs/2205.10487) 2022.05
* [Challenging BIG-Bench tasks and whether chain-of-thought can solve them](https://arxiv.org/abs/2210.09261) 2022.10
* [Let’s Verify Step by Step](https://arxiv.org/abs/2305.20050) 2023.05
* [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/abs/2304.06767) 2023.04
* [State of GPT](https://karpathy.ai/stateofgpt.pdf) 2023.05
* [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707) 2023.06
* [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267) 2023.09
* [Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259) 2023.08
* [Preference Ranking Optimization for Human Alignment](https://arxiv.org/abs/2306.17492) 2023.06
* [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 2022.03
* [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705) 2023.05
* [Instruction-Following Evaluation for Large Language Models](https://arxiv.org/abs/2311.07911) 2023.11
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) 2021.06
* [Some intuitions about large language models](https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/edit#slide=id.g16197112905_0_0) 2023.11
* [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) 2023.03
* [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066.pdf) 2024.01
* [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5) 2024.01
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) 2024.02
* [Gemini 1.0](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) 2023.12
* [Gemini 1.5](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) 2024.02
* [Weak-to-strong generalization](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf) 2023.12
* [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020) 2024.01
* [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) 2023.05

### Multimodality
* [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) 2024.02

