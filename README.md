# LLM Papers

## Compression
* [Compression for AGI](https://www.youtube.com/watch?v=dO4TPJkeaaU) 2023.02
* [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668) 2023.09

## Language
* [Language is primarily a tool for communication rather than thought](https://www.nature.com/articles/s41586-024-07522-w) 2024.06

## Scaling
* [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 2022.03
* [The Platonic Representation Hypothesis](https://arxiv.org/abs/2405.07987) 2024.05
* [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/) 2024.09.12
* [Parables on the Power of Planning in AI: From Poker to Diplomacy](https://www.youtube.com/watch?v=eaAonE58sLU) 2024.09.18
* [Don't teach. Incentivize.](https://www.youtube.com/watch?v=kYWUEV_e2ss) 2024.09.20
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf) 2025.01
* [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599) 2025.01
* [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495) 2025.04
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476) 2025.03
* [VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks](https://arxiv.org/abs/2504.05118) 2025.04

## Representation
* [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207) 2023.10

## Architecture
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 2017.06
* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) 2018.06
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) 2018.10
* [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066.pdf) 2024.01

## Alignment
* [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) 2022.10
* [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) 2023.03
* [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) 2023.04
* [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) 2023.05
* [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) 2023.05
* [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705) 2023.05
* [Preference Ranking Optimization for Human Alignment](https://arxiv.org/abs/2306.17492) 2023.06
* [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707) 2023.06
* [Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259) 2023.08
* [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020) 2024.01
* [From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification](https://arxiv.org/abs/2403.06326) 2024.03
* [From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models](https://arxiv.org/abs/2404.15846) 2024.04
* [Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models](https://arxiv.org/abs/2404.02823) 2024.04
* [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](https://arxiv.org/abs/2404.13208) 2024.04
* [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](https://arxiv.org/abs/2406.13542) 2024.06
* [Inverse Constitutional AI: Compressing Preferences into Principles](https://arxiv.org/abs/2406.06560#) 2024.06
* [Following Length Constraints in Instructions](https://arxiv.org/abs/2406.17744) 2024.06
* [LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387) 2025.02
* [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) 2022.12

## Scalable Oversight
* [Self-critiquing models for assisting human evaluators](https://arxiv.org/abs/2206.05802) 2022.06
* [Weak-to-strong generalization](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf) 2023.12
* [Prover-Verifier Games improve legibility of LLM outputs](https://arxiv.org/pdf/2407.13692) 2024.07

## Theorem Proving
* [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://www.arxiv.org/abs/2408.08152) 2024.08

## ICL
* [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) 2023.03
* [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018v1.pdf) 2024.04

## Math & Reasoning
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) 2022.01
* [Let’s Verify Step by Step](https://arxiv.org/abs/2305.20050) 2023.05
* [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201) 2023.05
* [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) 2023.05
* [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) 2023.12
* [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5) 2024.01
* [Large Language Models Can Learn Temporal Reasoning](https://arxiv.org/abs/2401.06853) 2024.01
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) 2024.02
* [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) 2024.03

## Tool
* [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332) 2021.12
* [Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution](https://arxiv.org/abs/2505.20286) 2025.05

## Report
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) 2023.07
* [Gemini 1.0](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) 2023.12
* [Gemini 1.5](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) 2024.02
* [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) 2024.07
* [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) 2024.12

## Talk & Blog
* [Distinguishing three alignment taxes](https://aligned.substack.com/p/three-alignment-taxes) 2022.12
* [State of GPT](https://karpathy.ai/stateofgpt.pdf) 2023.05
* [An Observation on Generalization](https://www.youtube.com/live/AKMuA_TVz3A) 2023.08
* [An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1: Pretraining](https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb#5673a54859eb42878098a21f1ba68085) 2023.09
* [Some intuitions about large language models](https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/edit#slide=id.g16197112905_0_0) 2023.11
* [MiniCPM：揭示端侧大语言模型的无限潜力](https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a) 2024.04
* [Llama 3 Opens the Second Chapter of the Game of Scale](https://yaofu.notion.site/Apr-2024-Llama-3-Opens-the-Second-Chapter-of-the-Game-of-Scale-efff1c0c185f4008af673b78faf83b61) 2024.04
* [Successful language model evals](https://www.jasonwei.net/blog/evals) 2024.05
* [OpenAI Model Spec](https://cdn.openai.com/spec/model-spec-2024-05-08.html) 2024.05
* [Claude’s Character](https://www.anthropic.com/news/claude-character) 2024.06
* [AI achieves silver-medal standard solving International Mathematical Olympiad problems](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) 2024.07
* [Three hypotheses on LLM reasoning](https://yaofu.notion.site/Three-hypotheses-on-LLM-reasoning-168e791bf30280a389d4deccbdb2fc83) 2024.12
* [Scaling Paradigms for Large Language Models](https://www.youtube.com/watch?v=yhpjpNXJDco) 2025.01
* [Detecting misbehavior in frontier reasoning models](https://openai.com/index/chain-of-thought-monitoring/) 2025.03
* [The Second Half](https://ysymyth.github.io/The-Second-Half/) 2025.04
* [Spurious Rewards: Rethinking Training Signals in RLVR](https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f) 2025.05
* [Introducing Kimi-Dev: A Strong and Open-source Coding LLM for Issue Resolution](https://moonshotai.github.io/Kimi-Dev/) 2025.06

## Physics of Language Models
* [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://arxiv.org/abs/2407.20311) 2024.07
* [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/abs/2408.16293) 2024.08
* [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316) 2023.09
* [Physics of Language Models: Part 3.2, Knowledge Manipulation](https://arxiv.org/abs/2309.14402) 2023.09
* [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/abs/2404.05405) 2024.04

## Evaluation
* [Killed by LLM](https://r0bk.github.io/killedbyllm/)
* [Challenging BIG-Bench tasks and whether chain-of-thought can solve them](https://arxiv.org/abs/2210.09261) 2022.10
* [COLLIE: Systematic Construction of Constrained Text Generation Tasks](https://arxiv.org/abs/2307.08689) 2023.07
* [FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation](https://arxiv.org/abs/2310.03214) 2023.10
* [FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models](https://arxiv.org/abs/2310.20410) 2023.10
* [Instruction-Following Evaluation for Large Language Models](https://arxiv.org/abs/2311.07911) 2023.11
* [GAIA: a benchmark for General AI Assistants](https://arxiv.org/abs/2311.12983) 2023.11
* [Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](https://arxiv.org/abs/2406.09170) 2024.06
* [Beyond Instruction Following: Evaluating Rule Following of Large Language Models](https://arxiv.org/abs/2407.08440) 2024.07
* [Introducing SimpleQA](https://openai.com/index/introducing-simpleqa/) 2024.10, [Measuring short-form factuality in large language models](https://arxiv.org/abs/2411.04368) 2024.11
* [Humanity's Last Exam](https://arxiv.org/abs/2501.14249) 2025.01
* [MultiChallenge](https://scale.com/leaderboard/multichallenge) 2025.03
* [BrowseComp: a benchmark for browsing agents](https://openai.com/index/browsecomp/) 2025.04
* [Introducing HealthBench](https://openai.com/index/healthbench/) 2025.05

## Quality
* [Scaling Laws and Interpretability of Learning from Repeated Data](https://arxiv.org/abs/2205.10487) 2022.05

## Efficient
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) 2021.06

## Merging
* [Evolutionary Optimization of Model Merging Recipes](https://sakana.ai/evolutionary-model-merge/) 2024.03

## Search
* [A Survey of Monte Carlo Tree Search Methods](http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf) 2012.03

# Multimodality Papers
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) 2020.10
* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) 2021.02
* [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) 2024.02
* [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) 2024.10
* [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) 2025.01

